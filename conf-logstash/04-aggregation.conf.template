##### MODIFY THIS FILE, IF NEEDED, THEN RENAME, REMOVING THE .template #####
##### OR COPY ANY CHANGES TO YOUR EXISTING VERSION AFTER AN UPGRADE    #####

# This filter stitches together flows from different nfcapd files, each (usually) spanning a 5 min. period.
# Note: netflow keeps the start time the same for all flows with the same fingerprint, even across different nfcapd files;
# duration is cumulative but counts are not. Sflow ends each flow as it is written out, as one would expect.
# If only 1 packet is seen, end time will = start time and duration will be 0.

filter {
  if [type] == 'flow' {

    # TSTAT - tstat only reports complete flows, so no stitching is needed!
    # Just add stitched_flows=0 (means no stitching attempted) and the fingerprint as meta.id
    if [meta][flow_type] == 'tstat' {
      # on tstat flows, just add the fields we would have had during aggregation
      mutate {
        add_field => { 'stitched_flows' => 0 }
        rename    => { 'flow_fingerprint' => '[meta][id]' }
      }

    } 

    # SFLOW AND NETFLOW - aggregate flows spanning more than 1 nfcapd file
    else {
      # make sure 'start' is a date (needed for aggregation filter)
      date {
          match  => [ '[start_timestamp]', 'UNIX' ]
          target => '[start]'
      }
 
      aggregate {
          # unique ID used to aggregate events
          task_id => '%{[flow_fingerprint]}'

          # save the fingerprint value as [meta][id] on timeout
          timeout_task_id_field => "[meta][id]"

          # use event's start time rather than system time to determine whether a timeout has occured (must be type 'date')
          timeout_timestamp_field => '[start]'

          # If more than inactivity_timeout seconds have passed between the 'start' of this event and the 'start'
          # of the LAST matching event, OR if no matching flow has coming in for inactivity_timeout seconds
          # on the clock, assume the flow has ended.
          ##  Use 630 sec = 10.5 min for 5-min files,  960 sec = 16 min for AMPATH which has has 15-min files.
          ##  (For 5-min files, this allows one 5 min gap or period during which the no. of bits transferred don't meet the cutoff)
          inactivity_timeout => 630
  
          # Maximum possible flow length. Stop aggregating even if we're still seeing matching events coming in.
          ##  Use 86400 sec = 1 day
          timeout => 86400

          # send the aggregation map as a new event upon timeout
          push_map_as_event_on_timeout => true
  
          # save the aggregation maps here in case logstash dies 
          ##  (use a different file for each logstash pipeline!)
          aggregate_maps_path => '/tmp/logstash-aggregation-maps'

          # ruby code to run each time we see an event
          code => "
            # keep track of how many events we aggregate
            map['stitched_flows'] ||= 0
            map['stitched_flows'] += 1

            # start and end times of the stitched flow
            # (Save only timestamps because date fields cause problems with the aggregate maps file, at least in 7.4.1)
            map['start_timestamp'] ||= event.get('start_timestamp')
            map['end_timestamp']   ||= event.get('end_timestamp')

            # save info from the first subflow
            # values will be updated as we stitch on other flows
            map['type']   ||= event.get('type')              # type will be 'flow' (vs macy, etc)
            map['meta']   ||= event.get('meta')
            map['values'] ||= event.get('values')

            # essentially the time the flow entered the pipeline
             map['@injest_time'] ||= Time.now     # saving @timestamp caused problems when agg map was saved to a file then read.
						  # @timestamp will be added when the map is finally pushed as an event.

       #### FOR TESTING
            # map['trial'] = 1
            # map['values']['durations_sum'] ||= 0;
            # map['values']['durations_sum'] += event.get('[values][duration]')
            # map['values']['durations'] ||= ' '
            # map['values']['durations'] += event.get('[values][duration]').to_s
            # map['values']['durations'] += '; '
       ####

            # if we are seeing a subsequent flow event
            if map['stitched_flows'] > 1

                # be very sure we are getting the correct start and end times, even if events are out of order
                map['start_timestamp'] = [ map['start_timestamp'], event.get('start_timestamp') ].min
                map['end_timestamp']   = [ map['end_timestamp'], event.get('end_timestamp') ].max
                
                # sum the packet and bit counters
                map['values']['num_packets'] += event.get('[values][num_packets]')
                map['values']['num_bits'] += event.get('[values][num_bits]')

                # recalculate total duration 
                map['values']['duration'] = map['end_timestamp'] - map['start_timestamp']

                # recalculate average pps and bps
                if map['values']['duration'] > 0
                    map['values']['packets_per_second'] = (map['values']['num_packets'] / map['values']['duration']).to_i;
                    map['values']['bits_per_second'] = (map['values']['num_bits'] / map['values']['duration']).to_i;
                else
                # can't calculate so set to 0 #
                    map['values']['packets_per_second'] = 0;
                    map['values']['bits_per_second'] = 0;
                end

                # round off after calculations
                map['values']['duration'] = (map['values']['duration']).round(3)

            end

            # discard the original event. we only care about the aggregation.
            event.cancel()
          "
      }
    }

  }
}
