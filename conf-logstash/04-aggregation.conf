filter {
  # only handle flow events
  if [type] == 'flow' {
    # create unique id for flow based on 5 data points
    # source/destination IP and port, and protocol
    fingerprint {
      source => [
        '[meta][src_ip]',
        '[meta][dst_ip]',
        '[meta][src_port]',
        '[meta][dst_port]',
        '[meta][protocol]',
        '[meta][sensor_id]'
      ]

      concatenate_sources => true
      method              => 'SHA256'
      target              => 'flow_fingerprint'
      key                 => 'create flowid'
      id                  => 'five-tuple-hash'
    }

    # we only need to aggregate non-tstat events
    if [meta][flow_type] == 'tstat' {
      # on tstat flows, just add the fields we would have had during aggregation
      mutate {
        add_field => { 'stitched_flows' => 0 }
        rename    => { 'flow_fingerprint' => '[meta][id]' }
      }
    } else {
      aggregate {
        # unique ID used to aggregate events
        task_id => '%{[flow_fingerprint]}'

        # save the fingerprint value as [meta][id] on timeout
        timeout_task_id_field => "[meta][id]"

        # use event start time rather than system time
        timeout_timestamp_field => '[start]'

        # stop aggregation if 5.5 minutes have passed since the last one
        inactivity_timeout => 330

        # stop aggregation if a day has passed and we're still seeing events
        timeout => 86400

        # send the aggregation map as a new event upon timeout
        push_map_as_event_on_timeout => true

        # save the aggregation maps here in case logstash dies
        aggregate_maps_path => '/tmp/aggregation-maps.logstash'

        # ruby code to run each time we see an event
        code => "
            # keep track of how many events we aggregate
            map['stitched_flows'] ||= 0
            map['stitched_flows'] += 1

            # save the first start time and the last end time
            map['start'] ||= event.get('start')
            map['end'] = event.get('end')

            # save the first start_timestamp and the last end_timestamp
            map['start_timestamp'] ||= event.get('start_timestamp')
            map['end_timestamp'] = event.get('end_timestamp')

            # save the time and interval
            map['type'] ||= event.get('type')
            map['interval'] ||= event.get('interval')

            # save the meta and values hashes
            map['meta'] ||= event.get('meta')
            map['values'] ||= event.get('values')

            # if we are seeing a subsequent flow event
            if map['stitched_flows'] > 1
              # sum the packet and bit counters
              map['values']['num_packets'] += event.get('[values][num_packets]')
              map['values']['num_bits'] += event.get('[values][num_bits]')

              # recalculate total duration as the difference between
              # the end of the last event and the start of first one
              map['values']['duration'] = (map['end_timestamp'] - map['start_timestamp']).round(3)

              # fix the averages for pps and bps
              map['values']['packets_per_second'] = (map['values']['num_packets'] / map['values']['duration']).to_i;
              map['values']['bits_per_second'] = (map['values']['num_bits'] / map['values']['duration']).to_i;
            end

            # discard the original event. we only care about the aggregation
            event.cancel()
        "
      }
    }
  }
}
