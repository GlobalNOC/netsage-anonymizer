filter {

  if [type] == "flow" {

  # Add a field to the event called "flow_fingerprint" (hashed 5-tuple) 
    fingerprint {
        source => [ '[meta][src_ip]', '[meta][dst_ip]', '[meta][src_port]', '[meta][dst_port]', '[meta][protocol]']
        concatenate_sources => true
        method => "SHA256"
        target => "flow_fingerprint"
        key => "create flowid"
        id  => "five-tuple-hash"
    }

  # TSTAT only reports complete flows, so no stitching is needed!
    if [meta][flow_type] == "tstat" {
        mutate {
            add_field => { "stitched_flows" => 0 }
            rename => { "flow_fingerprint" => "[meta][id]" }
        }
    } 

  # NETFLOW AND SFLOW
    else {

    # Aggregate, ie, stitch, events (individual raw flow records) with the same fingerprint. 
    # Raw sub-flows are, at most, 5 min long, 1 per nfcapd file, since each nfcapd file covers 5 min. and read with dfdump -a.
    # This actually pushes a "map" for each fingerprint as a new event and deletes the original events.
    # [NOTE: Number of logstash filter workers must be set to 1 for this filter to work correctly!]

        aggregate {
          # Match flows on 5-tuple-hash value. 
             task_id => "%{[flow_fingerprint]}"           

          # Use the "start" field in the flow records to check for timeouts instead of clock time. 
          # ("start" has to be type "date" - see 02-convert.conf)
             timeout_timestamp_field => 'start'

          # If more than inactivity_timeout seconds have passed since the START of the LAST match (comparing
          # $timeout_timestamp_field values), assume the flow has ended. 
          # (With inactivity_timeout = 5.5 min, 
          # a 5 min flow, then a gap of < .5 min, then another matching flow -> flows will be stitched.
          # a 1 sec flow at the end of a 5 min period, then a gap of < 5.5 min, then another matching flow' -> flows 
          # will be stitched.)
             inactivity_timeout => 330     # 330 sec = 5.5 min 

          # Max. time to wait to push the map since the FIRST match ~  maximum possible duration! 
             timeout =>  2592000           # 2592000 sec = 30 days

          # Push the aggregation map as a new event if either timeout occurs.
             push_map_as_event_on_timeout => true

          # Save the value of task_id to the final event as [meta][id]
             timeout_task_id_field => "[meta][id]"
 
          # Ruby code to execute when a matching event is found.
             code => "
                 if map['stitched_flows'].nil?
                 # If this is the first event with a specific flow_fingerprint, a map is created. 
                 # Add the event properties (it will become an event later).
                     map['stitched_flows'] = 1;
                     map['start'] = event.get('start');               
                     map['end']   = event.get('end');
                     map['type']  = event.get('type');        # =flow (vs macy, etc)
                     map['interval'] = event.get('interval'); # timespan for each nfcapd file???
                     map['meta']   = event.get('meta');        
                     map['values'] = event.get('values');
                  else
                  # If this is not the first event with a specific flow_fingerprint, update some of the event properties.
                      map['stitched_flows'] += 1;
                      map['start'] = [ map['start'], event.get('start') ].min;
                      map['end']  =  [ map['end'], event.get('end') ].max;
                      map['values']['duration']    +=  event.get('[values][duration]');
                      map['values']['num_packets'] +=  event.get('[values][num_packets]');
                      map['values']['num_bits']    +=  event.get('[values][num_bits]');
                      map['values']['packets_per_second'] = map['values']['num_packets'] / map['values']['duration'];        # ?????
                      map['values']['bits_per_second']    = map['values']['num_bits'] / map['values']['duration'];        # ?????
                  end
                "
    
          # File where the aggregate maps will be stored in case logstash dies. (If not set, they will be lost)
            aggregate_maps_path => '/tmp/logstash-aggregate-save'  # use only in production (causes errors when repeatably testing)
       }
    
      # Drop original events as we'll be keeping only maps pushed as events 
         if ![stitched_flows] {
             drop {}
         }

      }

  }

}
